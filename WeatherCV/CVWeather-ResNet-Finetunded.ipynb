{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Finetuning\n",
    "In the previous notebook, we trained a CNN from scratch and it performed poorly. In this notebook, we will use a pre-trained ResNet model and fine-tune it to our dataset. We will also use data augmentation to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomRotation, RandomVerticalFlip, ToTensor, Normalize, Resize, Compose, ColorJitter\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Check for GPU availability\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data folder and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.curdir, 'AllData')\n",
    "\n",
    "train_path = os.path.join(data_folder, 'train')\n",
    "test_path = os.path.join(data_folder, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    RandomHorizontalFlip(), # randomly flip and rotate\n",
    "    Resize((300, 400)), # resize to 300 x 400\n",
    "    RandomRotation(10), # randomly rotate by 10 degrees\n",
    "    ToTensor(), # convert to tensor\n",
    "    Normalize((0.5,), (0.5,)) # normalize the images\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((300, 400)),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images after augmentation: 5484\n",
      "Number of testing images after augmentation: 1378\n"
     ]
    }
   ],
   "source": [
    "train_data = ImageFolder(train_path, transform=train_transform)\n",
    "test_data = ImageFolder(test_path, transform=test_transform)\n",
    "\n",
    "print('Number of training images after augmentation: {}'.format(len(train_data)))\n",
    "print('Number of testing images after augmentation: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/omar/4CC8FF56C8FF3D30/ArabWeatherTask/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/omar/4CC8FF56C8FF3D30/ArabWeatherTask/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_classes = 11\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "n_splits = 5  # Number of cross-validation folds\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "\n",
    "val_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Fold 1, Epoch 1/20, Validation Accuracy: 80.67%\n",
      "Fold 1, Epoch 2/20, Validation Accuracy: 86.87%\n",
      "Fold 1, Epoch 3/20, Validation Accuracy: 89.15%\n",
      "Fold 1, Epoch 4/20, Validation Accuracy: 90.06%\n",
      "Fold 1, Epoch 5/20, Validation Accuracy: 90.98%\n",
      "Fold 1, Epoch 6/20, Validation Accuracy: 90.88%\n",
      "Fold 1, Epoch 7/20, Validation Accuracy: 91.80%\n",
      "Fold 1, Epoch 8/20, Validation Accuracy: 91.16%\n",
      "Fold 1, Epoch 9/20, Validation Accuracy: 91.80%\n",
      "Fold 1, Epoch 10/20, Validation Accuracy: 92.25%\n",
      "Fold 1, Epoch 11/20, Validation Accuracy: 92.25%\n",
      "Fold 1, Epoch 12/20, Validation Accuracy: 92.07%\n",
      "Fold 1, Epoch 13/20, Validation Accuracy: 92.25%\n",
      "Fold 1, Epoch 14/20, Validation Accuracy: 91.61%\n",
      "Fold 1, Epoch 15/20, Validation Accuracy: 91.80%\n",
      "Early stopping triggered.\n",
      "Fold 1, Test Accuracy: 93.40%\n",
      "Fold 2/5\n",
      "Fold 2, Epoch 1/20, Validation Accuracy: 92.16%\n",
      "Fold 2, Epoch 2/20, Validation Accuracy: 92.07%\n",
      "Fold 2, Epoch 3/20, Validation Accuracy: 91.98%\n",
      "Fold 2, Epoch 4/20, Validation Accuracy: 92.25%\n",
      "Fold 2, Epoch 5/20, Validation Accuracy: 92.53%\n",
      "Fold 2, Epoch 6/20, Validation Accuracy: 92.89%\n",
      "Fold 2, Epoch 7/20, Validation Accuracy: 92.25%\n",
      "Fold 2, Epoch 8/20, Validation Accuracy: 92.25%\n",
      "Fold 2, Epoch 9/20, Validation Accuracy: 92.53%\n",
      "Fold 2, Epoch 10/20, Validation Accuracy: 92.43%\n",
      "Fold 2, Epoch 11/20, Validation Accuracy: 92.53%\n",
      "Early stopping triggered.\n",
      "Fold 2, Test Accuracy: 92.96%\n",
      "Fold 3/5\n",
      "Fold 3, Epoch 1/20, Validation Accuracy: 92.53%\n",
      "Fold 3, Epoch 2/20, Validation Accuracy: 92.25%\n",
      "Fold 3, Epoch 3/20, Validation Accuracy: 92.16%\n",
      "Fold 3, Epoch 4/20, Validation Accuracy: 92.16%\n",
      "Fold 3, Epoch 5/20, Validation Accuracy: 92.34%\n",
      "Fold 3, Epoch 6/20, Validation Accuracy: 92.53%\n",
      "Early stopping triggered.\n",
      "Fold 3, Test Accuracy: 93.18%\n",
      "Fold 4/5\n",
      "Fold 4, Epoch 1/20, Validation Accuracy: 92.43%\n",
      "Fold 4, Epoch 2/20, Validation Accuracy: 92.43%\n",
      "Fold 4, Epoch 3/20, Validation Accuracy: 92.16%\n",
      "Fold 4, Epoch 4/20, Validation Accuracy: 91.98%\n",
      "Fold 4, Epoch 5/20, Validation Accuracy: 92.07%\n",
      "Fold 4, Epoch 6/20, Validation Accuracy: 91.89%\n",
      "Early stopping triggered.\n",
      "Fold 4, Test Accuracy: 93.03%\n",
      "Fold 5/5\n",
      "Fold 5, Epoch 1/20, Validation Accuracy: 91.97%\n",
      "Fold 5, Epoch 2/20, Validation Accuracy: 91.97%\n",
      "Fold 5, Epoch 3/20, Validation Accuracy: 92.24%\n",
      "Fold 5, Epoch 4/20, Validation Accuracy: 91.79%\n",
      "Fold 5, Epoch 5/20, Validation Accuracy: 92.24%\n",
      "Fold 5, Epoch 6/20, Validation Accuracy: 91.97%\n",
      "Fold 5, Epoch 7/20, Validation Accuracy: 91.97%\n",
      "Fold 5, Epoch 8/20, Validation Accuracy: 91.88%\n",
      "Early stopping triggered.\n",
      "Fold 5, Test Accuracy: 93.25%\n",
      "Average validation accuracy: 92.12%\n",
      "Average test accuracy: 93.16%\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in kf.split(range(len(train_data)), [label for (_, label) in train_data]):\n",
    "    fold += 1\n",
    "    print(f'Fold {fold}/{n_splits}')\n",
    "\n",
    "    # Split data into train and validation sets for this fold\n",
    "    train_fold, val_fold = random_split(\n",
    "        train_data, [len(train_index), len(val_index)], generator=torch.Generator().manual_seed(42)  \n",
    "    )\n",
    "\n",
    "    # Create data loaders for this fold\n",
    "    train_loader = DataLoader(train_fold, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_fold, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    early_stopping_patience = 5\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_weights = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "            \n",
    "    \n",
    "        # Calculate validation \n",
    "        model.eval()\n",
    "        correct_val= 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = correct_val / total_val * 100\n",
    "        print(f'Fold {fold}, Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            early_stopping_counter = 0\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # Evaluate on the test set for this fold\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    test_accuracy = correct_test / total_test * 100\n",
    "    print(f'Fold {fold}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "# Calculate and print average validation and test accuracies over all folds\n",
    "avg_val_accuracy = sum(val_accuracies) / n_splits\n",
    "avg_test_accuracy = sum(test_accuracies) / n_splits\n",
    "\n",
    "print(f'Average validation accuracy: {avg_val_accuracy:.2f}%')\n",
    "print(f'Average test accuracy: {avg_test_accuracy:.2f}%')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'resnet18_finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we have explored two different implementations of image classification using PyTorch. Image classification is a fundamental task in computer vision, where the goal is to categorize images into predefined classes or categories. We have used the popular deep learning framework PyTorch to build and train convolutional neural network (CNN) models for this task.\n",
    "\n",
    "## Code 1: Dataset Preparation and CNN Model Training\n",
    "\n",
    "In the first code implementation, we focused on the following key steps:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Unzipped and organized the dataset.\n",
    "   - Split the dataset into training, validation, and testing sets.\n",
    "   - Loaded and preprocessed the images using PyTorch's `ImageFolder` and applied data augmentation techniques.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Defined a custom CNN architecture for image classification.\n",
    "\n",
    "3. **Training and Evaluation**:\n",
    "   - Trained the model using a stratified k-fold cross-validation approach.\n",
    "   - Monitored and visualized training progress.\n",
    "   - Evaluated the model's performance on a separate test set.\n",
    "\n",
    "4. **Results Analysis**:\n",
    "   - Calculated and displayed average validation and test accuracies.\n",
    "   - Visualized sample images and class distributions.\n",
    "\n",
    "5. **Model Saving**:\n",
    "   - Saved the trained model for future use or deployment.\n",
    "\n",
    "## Code 2: Transfer Learning with ResNet18\n",
    "\n",
    "In the second code implementation, we used transfer learning with a pre-trained ResNet18 model:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Prepared the dataset in the same way as in the first code.\n",
    "\n",
    "2. **Model Selection**:\n",
    "   - Loaded a pre-trained ResNet18 model and adapted it for our classification task by modifying the fully connected layer.\n",
    "\n",
    "3. **Training and Evaluation**:\n",
    "   - Trained the modified model on the training dataset.\n",
    "   - Evaluated the model on the validation and test sets.\n",
    "\n",
    "4. **Learning Rate Scheduling**:\n",
    "   - Implemented a learning rate scheduler to dynamically adjust the learning rate during training.\n",
    "\n",
    "5. **Model Saving and Testing**:\n",
    "   - Saved the best-performing model's weights and tested it on the test dataset.\n",
    "\n",
    "Both implementations demonstrate different approaches to image classification, with the first code showcasing a custom CNN architecture and extensive data preprocessing, and the second code leveraging transfer learning with a pre-trained model to achieve impressive results.\n",
    "\n",
    "We will go with the fine-tuned ResNet for the deployment of our model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
